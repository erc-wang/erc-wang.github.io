<!DOCTYPE html>
<html>
<head>
    <title>My Portfolio</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
    </style>
</head>
<body>
    <h1>Project 5: Fun with Diffusion Models!</h1>
    <h3>About</h3>
    <p>This project is split into two parts with the first one covering usage of diffusion models and how to manipulate denoising processes to get results we want. The second part covers UNets and how to initialize their structure and models to denoise images.</p>
    <h1>Part A</h2>  
    <h3>Using Precomputed Text Embeddings</h3>
    <p>DeepFloyd is a text-to-image model that generates images based on text prompts. The prompt "a high quality photo" is used as a neutral, unconditional input, allowing the model to generate random images without specific guidance.</p>
    <p>This helps the model apply the diffusion process to transform noisy images into realistic ones. We also provide conditional inputs, which help determine the contents of the image that are generated.</p>
    <p>For this project I used seed = 3898</p>
    <p>The images below were generated, with their caption as their prompt and differing num_inferences.</p>
    <p>Stage 1 with num_inference_steps=20:</p>
    <table>
      <tr>
        <td>
          <img src="output/A/0a.png" width="256" height="256">
          <p>an oil painting of a snowy mountain village</p>
        </td>
        <td>
          <img src="output/A/0b.png" width="256" height="256">
          <p>a man wearing a hat</p>
        </td>
        <td>
          <img src="output/A/0c.png" width="256" height="256">
          <p>a rocket ship</p>
        </td>
      </tr>
    </table>
    <p>Stage 2 with num_inference_steps=20:</p>
    <table>
      <tr>
        <td>
          <img src="output/A/0d.png" width="256" height="256">
          <p>an oil painting of a snowy mountain village</p>
        </td>
        <td>
          <img src="output/A/0e.png" width="256" height="256">
          <p>a man wearing a hat</p>
        </td>
        <td>
          <img src="output/A/0f.png" width="256" height="256">
          <p>a rocket ship</p>
        </td>
      </tr>
    </table>
    <p>Stage 1 with num_inference_steps=10:</p>
    <table>
      <tr>
        <td>
          <img src="output/A/0g.png" width="256" height="256">
          <p>an oil painting of a snowy mountain village</p>
        </td>
        <td>
          <img src="output/A/0h.png" width="256" height="256">
          <p>a man wearing a hat</p>
        </td>
        <td>
          <img src="output/A/0i.png" width="256" height="256">
          <p>a rocket ship</p>
        </td>
      </tr>
    </table>
    <p>Stage 1 with num_inference_steps=30:</p>
    <table>
      <tr>
        <td>
          <img src="output/A/0j.png" width="256" height="256">
          <p>an oil painting of a snowy mountain village</p>
        </td>
        <td>
          <img src="output/A/0k.png" width="256" height="256">
          <p>a man wearing a hat</p>
        </td>
        <td>
          <img src="output/A/0l.png" width="256" height="256">
          <p>a rocket ship</p>
        </td>
      </tr>
    </table>
    <p>It is worth mentioning that every image has been upscaled to 256x256 when every stage 1 output is actually 64x64. For later parts of this project, all outputs will be originally 64x64.</p>
    <p>The outputs to seem to heavily reflect the prompt that was inserted into the model. Stage 2 takes significantly longer to run as the images are exponentially larger in size.</p>
    <p>When num_inference_steps increases, the edges become more defined and the finer details can be made out. When num_inference_steps is lower, there is largely more visible amount of blurring.</p>
    <h3>Sampling Loops</h3>
    <p>In a diffusion model, we can start with a clean image and progressively add noise over time until it becomes pure noise. The model reverses this process by predicting and removing the noise, gradually denoising the image. </p> 
    <p>We can also generate images, by starting with pure noise and iteratively removing it, refining the image step by step until a clean image appears. We can manipulate the amount of noise that is added to each step.</p>
    <h3>Implementing the Forward Process</h3>
    <p>The forward process entails of noising an image from a Gaussian distribution with a specific mean and variance at each timestep.</p>
    <p>The variable alphas_cumprod tracks the noise level, where smaller t values correspond to cleaner images, and larger t values indicate more noise.</p>
    <p>The function forward(im, t) depicts the image im at step t.</p>
    <table>
      <tr>
        <td>
          <img src="output/A/campanile.png" width="256" height="256">
          <p>Berkeley Campanile</p>
        </td>
        <td>
          <img src="output/A/11a.png" width="256" height="256">
          <p>Noisy Campanile at t=250</p>
        </td>
        <td>
          <img src="output/A/11b.png" width="256" height="256">
          <p>Noisy Campanile at t=500</p>
        </td>
        <td>
          <img src="output/A/11c.png" width="256" height="256">
          <p>Noisy Campanile at t=750</p>
        </td>
      </tr>
    </table>
    <h3>Classical Denoising</h3>
    <p>A strategy we can employ to denoise an image is to use Gaussian blur filtering. To do this we simply take the image and insert it into the built in torch gaussian_blur function which will essentially smoothen out the noise.</p>
    <p>This method will prove to be not super accurate, as it is bound to lose information and has no real way of making it up.</p>
    <table>
      <tr>
        <td>
          <img src="output/A/11a.png" width="256" height="256">
          <p>Noisy Campanile at t=250</p>
        </td>
        <td>
          <img src="output/A/11b.png" width="256" height="256">
          <p>Noisy Campanile at t=500</p>
        </td>
        <td>
          <img src="output/A/11c.png" width="256" height="256">
          <p>Noisy Campanile at t=750</p>
        </td>
      </tr>
    </table>
    <table>
      <tr>
        <td>
          <img src="output/A/12a.png" width="256" height="256">
          <p>Gaussian Denoise at t=250</p>
        </td>
        <td>
          <img src="output/A/12b.png" width="256" height="256">
          <p>Gaussian Denoise at t=500</p>
        </td>
        <td>
          <img src="output/A/12c.png" width="256" height="256">
          <p>Gaussian Denoise at t=750</p>
        </td>
      </tr>
    </table>
    <h3>One-Step Denoising</h3>
    <p>We can train the diffusion model with a UNet to denoise the image. The UNet is conditioned on a noise level and uses a timestep as an additional input to predict and remove Gaussian noise to recover images closer to the original.</p>
    <p>Since the model was trained with text conditioning we also need a text prompt embedding, which is provided for the prompt "a high quality photo".</p>
    <table>
      <tr>
        <td>
          <img src="output/A/11a.png" width="256" height="256">
          <p>Noisy Campanile at t=250</p>
        </td>
        <td>
          <img src="output/A/11b.png" width="256" height="256">
          <p>Noisy Campanile at t=500</p>
        </td>
        <td>
          <img src="output/A/11c.png" width="256" height="256">
          <p>Noisy Campanile at t=750</p>
        </td>
      </tr>
    </table>
    <table>
      <tr>
        <td>
          <img src="output/A/13a.png" width="256" height="256">
          <p>One-Step at t=250</p>
        </td>
        <td>
          <img src="output/A/13b.png" width="256" height="256">
          <p>One-Step at t=500</p>
        </td>
        <td>
          <img src="output/A/13c.png" width="256" height="256">
          <p>One-Step at t=750</p>
        </td>
      </tr>
    </table>
    <img src="output/A/campanile.png" width="256" height="256">
    <p>Original Campanile</p>
    <h3>Iterative Denoising</h3>
    <p>From the previous part we can see that UNet is significantly more successful at denoising the image compared to standard Gaussian blur filtering. However it still fails to denoise when their is an excessive amount of noise.</p>
    <p>A way we can counteract this is by implementing iterative denoising. We can specify timesteps where we would denoise the image at a certain timestep and use information from the denoised image at the previous timestep and the initial image.</p>
    <p>We can skip steps to make the process faster and essentially runs the previous process multiple times using other previous output.</p>
    <table>
      <tr>
        <td>
          <img src="output/A/14e.png" width="256" height="256">
          <p>Noisy Campanile at t=90</p>
        </td>
        <td>
          <img src="output/A/14d.png" width="256" height="256">
          <p>Noisy Campanile at t=240</p>
        </td>
        <td>
          <img src="output/A/14c.png" width="256" height="256">
          <p>Noisy Campanile at t=390</p>
        </td>
        <td>
          <img src="output/A/14b.png" width="256" height="256">
          <p>Noisy Campanile at t=540</p>
        </td>
        <td>
          <img src="output/A/14a.png" width="256" height="256">
          <p>Noisy Campanile at t=690</p>
        </td>
      </tr>
    </table>
    <table>
      <tr>
        <td>
          <img src="output/A/campanile.png" width="256" height="256">
          <p>Original</p>
        </td>
        <td>
          <img src="output/A/14g.png" width="256" height="256">
          <p>Iteratively Denoised</p>
        </td>
        <td>
          <img src="output/A/14h.png" width="256" height="256">
          <p>One-Step Denoised</p>
        </td>
          <td>
          <img src="output/A/14i.png" width="256" height="256">
          <p>Gaussian Blurred</p>
        </td>
      </tr>
    </table>
    <h3>Diffusion Model Sampling</h3>
    <p>We can essentially generate random images by inserting random noise as the initial image. We set i_start to 0 and set the prompt to "a high quality photo".</p>
    <table>
      <tr>
        <td>
          <img src="output/A/15a.png" width="256" height="256">
          <p>Sample 1</p>
        </td>
        <td>
          <img src="output/A/15b.png" width="256" height="256">
          <p>Sample 2</p>
        </td>
        <td>
          <img src="output/A/15c.png" width="256" height="256">
          <p>Sample 3</p>
        </td>
          <td>
          <img src="output/A/15d.png" width="256" height="256">
          <p>Sample 4</p>
        </td>
        </td>
          <td>
          <img src="output/A/15e.png" width="256" height="256">
          <p>Sample 5</p>
        </td>
      </tr>
    </table>
    <h3>Classifier Free Guidance (CFG)</h3>
    <p>The randomly generated images from the previous task were not very good in quality and lack substance. We can utilize CFG to greatly improve the quality of the images at the cost of image diversity.</p>
    <p>We can calculate estimated noise e = e_uncond + gamma * (e_cond - e_uncond)</p>
    <p>Where gamma controls the strength of the CFG.</p>
    <p>In the following examples we set "" as the unconditional prompt embed, and leave the prompt embed as "a high quality photo".</p>
    <table>
      <tr>
        <td>
          <img src="output/A/16a.png" width="256" height="256">
          <p>Sample 1 with CFG</p>
        </td>
        <td>
          <img src="output/A/16b.png" width="256" height="256">
          <p>Sample 2 with CFG</p>
        </td>
        <td>
          <img src="output/A/16c.png" width="256" height="256">
          <p>Sample 3 with CFG</p>
        </td>
          <td>
          <img src="output/A/16d.png" width="256" height="256">
          <p>Sample 4 with CFG</p>
        </td>
        </td>
          <td>
          <img src="output/A/16e.png" width="256" height="256">
          <p>Sample 5 with CFG</p>
        </td>
      </tr>
    </table>
    <h3>Image-to-image Translation</h3>
    <p>As we can see with the process of generating random images. The more noise there is an image, the more liberty the Diffusion model has in deciding what the final image will look like.</p>
    <p>If we run the iterative denoiser by inserting the original image forwarded to different noise levels, we can get a range of images, gradually beginning to look like the original.</p>
    <p>This method is labeled as the SDEdit algorithm which takes the original image, noises it a little, and then forces it back into the image without conditioning.</p>
    <table>
      <tr>
        <td>
          <img src="output/A/17b.png" width="256" height="256">
          <p>SDEdit with i_start=1</p>
        </td>
        <td>
          <img src="output/A/17c.png" width="256" height="256">
          <p>SDEdit with i_start=3</p>
        </td>
        <td>
          <img src="output/A/17d.png" width="256" height="256">
          <p>SDEdit with i_start=5</p>
        </td>
          <td>
          <img src="output/A/17e.png" width="256" height="256">
          <p>SDEdit with i_start=7</p>
        </td>
        </td>
          <td>
          <img src="output/A/17f.png" width="256" height="256">
          <p>SDEdit with i_start=10</p>
        </td>
        </td>
          <td>
          <img src="output/A/17g.png" width="256" height="256">
          <p>SDEdit with i_start=20</p>
        </td>
        </td>
          <td>
          <img src="output/A/campanile.png" width="256" height="256">
          <p>Campanile</p>
        </td>
      </tr>
    </table>
    <h3>Editing Hand-Drawn and Web Images</h3>
    <p>Following the same SDEdit algorithm, we can experiment with the creativity of the diffusion model by inserting images from the web and hand-drawn images that may not resemble anything at all.</p>
    <p>The Diffusion model will denoise these and create something from seemingly nothing.</p>
    <table>
      <tr>
        <td>
          <img src="output/A/171b.png" width="256" height="256">
          <p>SDEdit with i_start=1</p>
        </td>
        <td>
          <img src="output/A/171c.png" width="256" height="256">
          <p>SDEdit with i_start=3</p>
        </td>
        <td>
          <img src="output/A/171d.png" width="256" height="256">
          <p>SDEdit with i_start=5</p>
        </td>
          <td>
          <img src="output/A/171e.png" width="256" height="256">
          <p>SDEdit with i_start=7</p>
        </td>
        </td>
          <td>
          <img src="output/A/171f.png" width="256" height="256">
          <p>SDEdit with i_start=10</p>
        </td>
        </td>
          <td>
          <img src="output/A/171g.png" width="256" height="256">
          <p>SDEdit with i_start=20</p>
        </td>
        </td>
          <td>
          <img src="output/A/171a.png" width="256" height="256">
          <p>Donkey Kong</p>
        </td>
      </tr>
    </table>
    <table>
      <tr>
        <td>
          <img src="output/A/171i.png" width="256" height="256">
          <p>SDEdit with i_start=1</p>
        </td>
        <td>
          <img src="output/A/171j.png" width="256" height="256">
          <p>SDEdit with i_start=3</p>
        </td>
        <td>
          <img src="output/A/171k.png" width="256" height="256">
          <p>SDEdit with i_start=5</p>
        </td>
          <td>
          <img src="output/A/171l.png" width="256" height="256">
          <p>SDEdit with i_start=7</p>
        </td>
        </td>
          <td>
          <img src="output/A/171m.png" width="256" height="256">
          <p>SDEdit with i_start=10</p>
        </td>
        </td>
          <td>
          <img src="output/A/171n.png" width="256" height="256">
          <p>SDEdit with i_start=20</p>
        </td>
        </td>
          <td>
          <img src="output/A/171h.png" width="256" height="256">
          <p>Original Sketch</p>
        </td>
      </tr>
    </table>
    <table>
      <tr>
        <td>
          <img src="output/A/171p.png" width="256" height="256">
          <p>SDEdit with i_start=1</p>
        </td>
        <td>
          <img src="output/A/171q.png" width="256" height="256">
          <p>SDEdit with i_start=3</p>
        </td>
        <td>
          <img src="output/A/171r.png" width="256" height="256">
          <p>SDEdit with i_start=5</p>
        </td>
          <td>
          <img src="output/A/171s.png" width="256" height="256">
          <p>SDEdit with i_start=7</p>
        </td>
        </td>
          <td>
          <img src="output/A/171t.png" width="256" height="256">
          <p>SDEdit with i_start=10</p>
        </td>
        </td>
          <td>
          <img src="output/A/171u.png" width="256" height="256">
          <p>SDEdit with i_start=20</p>
        </td>
        </td>
          <td>
          <img src="output/A/171o.png" width="256" height="256">
          <p>Original Sketch</p>
        </td>
      </tr>
    </table>
    <h3>Inpainting</h3>
    <p>We can use a binary mask that overlays the original image to determine a section(s) of the original image that we want to remain consistent, whereas the rest is up to change.</p>
    <p>This can be done by doing the standard iterative denoising process, however at the end of every loop we force only the masked pixels to be changed from the original image. </p>
    <table>
        <tr>
            <td>
                <img src="output/A/172a.png" width="256" height="256">
                <p>Campanile</p>
            </td>
            <td>
                <img src="output/A/172b.png" width="256" height="256">
                <p>Mask</p>
            </td>
            <td>
                <img src="output/A/172c.png" width="256" height="256">
                <p>Hole to Fill</p>
            </td>
            <td>
                <img src="output/A/172g.png" width="256" height="256">
                <p>Campanile Inpainted</p>
            </td>
        </tr>
    </table>
    <p>The colors tend to become a bit distorted as the process continues as it tries to iterate on only the noisy location which is separate from the rest of the image that is not masked.</p>
    <p>As a result we also need to run this process a few times in order to produce desirable results.</p>
    <table>
        <tr>
            <td>
                <img src="output/A/172i.png" width="256" height="256">
                <p>Meme</p>
            </td>
            <td>
                <img src="output/A/172j.png" width="256" height="256">
                <p>Mask</p>
            </td>
            <td>
                <img src="output/A/172k.png" width="256" height="256">
                <p>Hole to Fill</p>
            </td>
            <td>
                <img src="output/A/172l.png" width="256" height="256">
                <p>Meme Inpainted</p>
            </td>
        </tr>
    </table>
    <table>
        <tr>
            <td>
                <img src="output/A/172m.png" width="256" height="256">
                <p>Laptop</p>
            </td>
            <td>
                <img src="output/A/172n.png" width="256" height="256">
                <p>Mask</p>
            </td>
            <td>
                <img src="output/A/172o.png" width="256" height="256">
                <p>Hole to Fill</p>
            </td>
            <td>
                <img src="output/A/172q.png" width="256" height="256">
                <p>Laptop Inpainted</p>
            </td>
        </tr>
    </table>
    <p>The meme example is honestly really not great, but I was curious how well the Diffusion model and the mask would work on an area that literally has no information within it.</p>
    <p>Color artifacts can clearly be seen where the masked area is shades darker than the rest of the image, however the affects of the white color can be seen in the fact that the inpainting is entirely grayscale.</p>
    <h3>Text-Conditional Image-to-image Translation</h3>
    <p>We can perform SDEdit as seen earlier but with a prompt conditional. By changing "a high quality photo" with something like "a rocket ship", we can greatly influence the output.</p>
    <table>
      <tr>
        <td>
          <img src="output/A/173a.png" width="256" height="256">
          <p>Rocket Ship at noise level 1</p>
        </td>
        <td>
          <img src="output/A/173b.png" width="256" height="256">
          <p>Rocket Ship at noise level 3</p>
        </td>
        <td>
          <img src="output/A/173c.png" width="256" height="256">
          <p>Rocket Ship at noise level 5</p>
        </td>
          <td>
          <img src="output/A/173d.png" width="256" height="256">
          <p>Rocket Ship at noise level 7</p>
        </td>
        </td>
          <td>
          <img src="output/A/173e.png" width="256" height="256">
          <p>Rocket Ship at noise level 10</p>
        </td>
        </td>
          <td>
          <img src="output/A/173f.png" width="256" height="256">
          <p>Rocket Ship at noise level 20</p>
        </td>
        </td>
          <td>
          <img src="output/A/campanile.png" width="256" height="256">
          <p>Campanile</p>
        </td>
      </tr>
    </table>
    <table>
      <tr>
        <td>
          <img src="output/A/173g.png" width="256" height="256">
          <p>Rocket Ship at noise level 1</p>
        </td>
        <td>
          <img src="output/A/173h.png" width="256" height="256">
          <p>Rocket Ship at noise level 3</p>
        </td>
        <td>
          <img src="output/A/173i.png" width="256" height="256">
          <p>Rocket Ship at noise level 5</p>
        </td>
          <td>
          <img src="output/A/173j.png" width="256" height="256">
          <p>Rocket Ship at noise level 7</p>
        </td>
        </td>
          <td>
          <img src="output/A/173k.png" width="256" height="256">
          <p>Rocket Ship at noise level 10</p>
        </td>
        </td>
          <td>
          <img src="output/A/173l.png" width="256" height="256">
          <p>Rocket Ship at noise level 20</p>
        </td>
        </td>
          <td>
          <img src="output/A/171a.png" width="256" height="256">
          <p>Donkey Kong</p>
        </td>
      </tr>
    </table>
    <table>
      <tr>
        <td>
          <img src="output/A/173n.png" width="256" height="256">
          <p>Rocket Ship at noise level 1</p>
        </td>
        <td>
          <img src="output/A/173o.png" width="256" height="256">
          <p>Rocket Ship at noise level 3</p>
        </td>
        <td>
          <img src="output/A/173p.png" width="256" height="256">
          <p>Rocket Ship at noise level 5</p>
        </td>
          <td>
          <img src="output/A/173q.png" width="256" height="256">
          <p>Rocket Ship at noise level 7</p>
        </td>
        </td>
          <td>
          <img src="output/A/173r.png" width="256" height="256">
          <p>Rocket Ship at noise level 10</p>
        </td>
        </td>
          <td>
          <img src="output/A/173s.png" width="256" height="256">
          <p>Rocket Ship at noise level 20</p>
        </td>
        </td>
          <td>
          <img src="output/A/173m.png" width="256" height="256">
          <p>Pencils</p>
        </td>
      </tr>
    </table>
    <h3>Visual Anagrams</h3>
    <p>We can create optical illusions where an image resembles one prompt when seen upright, and another prompt when seen upside down.</p>
    <p>We can do this by getting the noise estimate of the image upright and upside down utilizing the UNet.</p>
    <p>We can perform CFG accordingly on these prompts in both directions and get the average of these noise estimates.</p>
    <p>Using this final noise estimate we can produce these anagrams.</p>
    <p>an image of "an oil painting of people around a campfire" and "an oil painting of an old man" when flipped</p>
    <table>
        <tr>
            <td>
                <p>An Oil Painting of People around a Campfire</p>
                <img src="output/A/18a.png" width="256" height="256">
            </td>
            <td>
                <img src="output/A/18b.png" width="256" height="256">
                <p>An Oil Painting of an Old Man</p>
            </td>
        </tr>
    </table>
    <p>an image of "a photo of a hipster barista" and "an oil painting of an old man" when flipped</p>
    <table>
        <tr>
            <td>
                <p>A Photo of a Hipster Barista</p>
                <img src="output/A/18c.png" width="256" height="256">
            </td>
            <td>
                <img src="output/A/18d.png" width="256" height="256">
                <p>An Oil Painting of an Old Man</p>
            </td>
        </tr>
    </table>
    <p>an image of "a photo of the amalfi cost" and "a photo of a dog" when flipped</p>
    <table>
        <tr>
            <td>
                <p>A Photo of the Amalfi Cost</p>
                <img src="output/A/18e.png" width="256" height="256">
            </td>
            <td>
                <img src="output/A/18f.png" width="256" height="256">
                <p>A Photo of a Dog</p>
            </td>
        </tr>
    </table>
    <h3>Hybrid Images</h3>
    <p>We can use a similar process to visual anagrams to create hybrid images like in project 2. We can get the noise estimates from all prompts and then insert one into a low pass filter and the other into a high pass filter and add them together to get our noise estimate.</p>
    <table>
        <tr>
            <td>
                <img src="output/A/19sub2.png" width="256" height="256">
                <p>Hybrid Image of a Skull and a Waterfall</p>
            </td>
            <td>
                <img src="output/A/19c.png" width="256" height="256">
                <p>Hybrid Image of an Old Man and a Snowy Mountain Village</p>
            </td>
            <td>
                <img src="output/A/19d.png" width="256" height="256">
                <p>Hybrid Image of an Old Man and People Around a Campfire</p>
            </td>
        </tr>
    </table>
</body>
</html>
